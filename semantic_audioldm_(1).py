# -*- coding: utf-8 -*-
"""Semantic_audioldm (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p_qHfJP9fo31Jekti_OnN2fBXD42_L2L
"""

import pandas as pd
import csv
import numpy as np
import requests
import scipy
from scipy.signal import lfilter
import matplotlib.pyplot as plt

!pip install openl3
import openl3
import soundfile as sf

# Define the URL of the CSV file
csv_url = 'https://raw.githubusercontent.com/azadehn/AudioGen/main/audioGen_dataset.csv'

# Read the CSV file from the provided URL
df = pd.read_csv(csv_url)

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile

def get_audios(temp_dir):
    # Local file paths for the zip files
    zip_file_paths = [
        '/content/drive/MyDrive/2024/11.november/audio/audio_files/1_1000.zip',
        '/content/drive/MyDrive/2024/11.november/audio/audio_files/1001_2000.zip'
    ]

    audio_files = []  # To store extracted audio file paths

    for zip_path in zip_file_paths:
        # Extract the zip file
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)

        # Get all the audio file paths from the extracted location
        extracted_files = os.listdir(temp_dir)
        audio_files.extend([os.path.join(temp_dir, file) for file in extracted_files if file.endswith('.wav')])

    return audio_files


temp_dir = '/content/drive/MyDrive/2024/11.november/audio/audio_files'
audio_files = get_audios(temp_dir)
print(audio_files)

import os

def get_audios_already_unzipped(temp_dir):

    audio_files = []  # To store extracted audio file paths
    files_list = os.listdir(temp_dir)
    audio_files.extend([os.path.join(temp_dir, file) for file in files_list if file.endswith('.wav')])
    #print(audio_files)
    return audio_files

#temp_dir = '/content/drive/MyDrive/2024/11.november/audio/audio_files'
#audio_files = get_audios_already_unzipped(temp_dir)
#print(audio_files)

def load_audio_data(row):
    temp_dir = '/content/drive/MyDrive/2023/11.november/audio/audio_files'
    os.makedirs(temp_dir, exist_ok=True)

    filename = row["filename"]
    audio_paths = get_audios_already_unzipped(temp_dir)#get_audios(temp_dir)

    # Find the path corresponding to the given filename
    audio_path = next((path for path in audio_paths if os.path.basename(path) == filename), None)
    #print(audio_path)
    y, sr = sf.read(audio_path)
    return y, sr

print(df.head())

import numpy as np
import os

def load_embeddings_ts(folder):
  embedding_ts = []
  print(folder)
  files = os.listdir(folder)
  print(files)

  for index, row in df.iterrows():
    fnNPY = f"embedding_{row['filename']}.npy"
    if fnNPY in files:
      emb = np.load(f'{folder}/embedding_{row["filename"]}.npy')
      ts = np.load(f'{folder}/timestap_{row["filename"]}.npy')
      embedding_ts.append((emb, ts, row['filename']))
      print(row)
  return embedding_ts

def save_embeddings_ts(df, embedding_ts):
  for index, row in df.iterrows():
    if row['filename'] not in embedding_ts:
      y, sr = load_audio_data(row)
      emb, ts= openl3.get_audio_embedding(y, sr)
      np.save(f'/content/drive/MyDrive/2023/11.november/audio/embeddings_timestamps/embedding_{row["filename"]}.npy', emb)
      np.save(f'/content/drive/MyDrive/2023/11.november/audio/embeddings_timestamps/timestamp_{row["filename"]}.npy', ts)
    else:
      print(f'{row["filename"]} already exists')

embedding_ts = load_embeddings_ts('/content/drive/MyDrive/2023/11.november/audio/embeddings_timestamps')
save_embeddings_ts(df, embedding_ts)
#print(embedding_ts)

def check_third_element_against_filename(tuples_list, filename):
    # Check if the list is empty
    if not tuples_list:
        return False

    # Iterate over each tuple in the list
    for t in tuples_list:
        try:
            if t[2] != filename:
                return False
        except IndexError:
            # If a tuple doesn't have a third element
            return False

    return True

# Example usage
tuples_list = [('item1', 'data1', 'test.txt'), ('item2', 'data2', 'test.txt'), ('item3', 'data3', 'test.txt')]
filename = 'test.txt'
print(check_third_element_against_filename(tuples_list, filename))  # Should return True if all third elements match 'test.txt'

for element in embedding_ts:
  print(element[2])

import numpy as np
import os


def comprehensive_matrix(df):

    # Initialize empty list to store feature vectors
    features = []

    # Function to extract features from a single row
    #def extract_features(row):
    # Define the start index
    start_index = 0

    # Check if the DataFrame has enough rows
    if start_index < len(df):
      # Iterate from row 1000 onwards
      for index, row in df.iloc[start_index:].iterrows():
        print(f"Row {index}: {row.to_dict()}")
        y, sr = load_audio_data(row)
        #print(row)
        emb, ts= openl3.get_audio_embedding(y, sr)
        np.save(f'/content/drive/MyDrive/2023/11.november/audio/embeddings_timestamps/embedding_{row["filename"]}.npy', emb)
        np.save(f'/content/drive/MyDrive/2023/11.november/audio/embeddings_timestamps/timestap_{row["filename"]}.npy', ts)

        emb_mean = np.mean(emb, axis=1)
        emb_variance = np.var(emb, axis=1)
        #emb_deltas = np.diff(emb, n=1, axis=1)
        #delta_mean = np.mean(emb_deltas, axis=1)
        #emb_delta_deltas = np.diff(emb, n=2, axis=1)
        #delta_delta_mean = np.mean(emb_delta_deltas, axis=1)

        # Append the results to the respective lists
        feature_vector = np.concatenate((emb_mean, emb_variance))

        features.append(feature_vector)

    else:
        print("Not enough rows in the DataFrame")

    # Convert the lists of feature vectors to a feature matrix
    comprehensive_matrix = np.array(features)

    return comprehensive_matrix


# Call the function
comprehensive_matrix = comprehensive_matrix(df)


# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)


print(comprehensive_matrix.shape)


saved_matrix = pd.DataFrame(comprehensive_matrix)
saved_matrix.to_csv('comprehensive_matrix.csv', index=False)

from google.colab import files

# Provide the file name you want to download
files.download('comprehensive_matrix.csv')

!pip install scikit-learn
from sklearn.decomposition import PCA

def scree_plot(comprehensive_matrix, num_components=None):

    # Perform PCA
    pca = PCA()
    pca.fit(comprehensive_matrix)

    explained_variace = np.cumsum(pca.explained_variance_ratio_)

    if num_components is not None:
        explained_variace = explained_variace[:num_components]

    # Plot the scree plot
    plt.plot(range(1, len(explained_variace) + 1), explained_variace, marker='o', linestyle='-')
    plt.xlabel('Principal Component Number')
    plt.ylabel('Eigenvalue')
    plt.title('Scree Plot')
    plt.grid()
    plt.show()


# To plot all components:
scree_plot(comprehensive_matrix)

def pca(comprehensive_matrix, num_components=None):

    # Initialize PCA
    pca = PCA(n_components=num_components,whiten = True)

    # Fit the PCA model to the data and transform the data
    reduced_features = pca.fit_transform(comprehensive_matrix)

    return reduced_features


# Call the function to run PCA on comprehensive matrix
reduced_matrix = pca(comprehensive_matrix, num_components=40)

# Print the matrix
print("Reduced Matrix:")
print(reduced_matrix)

reduced_matrix.shape

def total_variance(df, reduced_matrix, class_names):

    if not isinstance(class_names, list):
        class_names = [class_names]

    tv = np.trace(np.cov(reduced_matrix[df['category'].isin(class_names)].T))

    return tv

class_name = ["dog", "rain"]
result = total_variance(df, reduced_matrix, class_name)
print(result)

def avg_pairwise_distance(df, reduced_matrix, class_names):

    if not isinstance(class_names, list):
        class_names = [class_names]

    pdist = scipy.spatial.distance.pdist(reduced_matrix[df['category'].isin(class_names)])
    avg_pdist = np.mean(pdist)

    return avg_pdist

class_name = ["dog", "rain"]
result = avg_pairwise_distance(df, reduced_matrix, class_name)
print(result)

def generalized_variance(df, reduced_matrix, class_names):

    if not isinstance(class_names, list):
        class_names = [class_names]

    gv = np.linalg.det(np.cov(reduced_matrix[df['category'].isin(class_names)].T))

    return gv

class_name = ["dog", "rain"]
result = generalized_variance(df, reduced_matrix, class_name)
print(result)

import scipy.spatial.distance

names = ['dog', 'rooster', 'pig', 'cow', 'frog', 'cat', 'hen', 'insects', 'sheep', 'crow',
         'rain', 'sea_waves', 'crackling_fire', 'crickets', 'chirping_birds', 'water_drops', 'wind', 'pouring_water', 'toilet_flush', 'thunderstorm',
         'crying_baby', 'sneezing', 'clapping', 'breathing', 'coughing', 'footsteps', 'laughing', 'brushing_teeth', 'snoring', 'drinking_sipping',
         'door_wood_knock', 'mouse_click', 'keyboard_typing', 'door_wood_creaks', 'can_opening', 'washing_machine', 'vacuum_cleaner', 'clock_alarm', 'clock_tick', 'glass_breaking',
         'helicopter', 'chainsaw', 'siren', 'car_horn', 'engine', 'train', 'church_bells', 'airplane', 'fireworks', 'hand_saw'
         ]

def metrics(reduced_matrix, names):

    results_matrix = []

    for name in names:
        tv = total_variance(df, reduced_matrix, name)
        gv = generalized_variance(df, reduced_matrix, name)
        avg_pdist = avg_pairwise_distance(df, reduced_matrix, name)

        results_matrix.append([name, tv, gv, avg_pdist])
        metrics_matrix = np.array(results_matrix)

    return metrics_matrix


# Call the function
metrics_matrix = metrics(reduced_matrix, names)

# Print the matrix
print("Metrics Matrix:")
print(metrics_matrix)

metrics_matrix.shape



saved_metrics = pd.DataFrame(metrics_matrix)
saved_metrics.to_csv('metrics_matrix.csv', index=False)

from google.colab import files

# Provide the file name you want to download
files.download('metrics_matrix.csv')