# Evaluation_of_Audio_Generative_Models

Generative Audio Models

Generative audio modeling is an emerging area of generative AI that aims to synthesize realistic audio such as speech, music, and environmental sounds. Compared to text and vision, audio generation is more challenging due to long-range temporal dependencies, high-dimensional continuous signals, and the need to model multi-scale structure in waveforms and spectrograms. Recent progress in neural audio synthesis, self-supervised audio representation learning, and transformer-based language modeling has enabled high-quality audio generation using approaches such as autoregressive modeling, GANs, and diffusion models. Many state-of-the-art systems still rely on strong conditioning (e.g., text prompts, melody, genre, or artist attributes) to generate coherent and high-fidelity samples.

This section summarizes key families of generative audio models (e.g., AudioGen, Make-An-Audio, AudioLDM) and outlines common evaluation strategies for audio generation, including objective metrics (FD, FAD, IS, KLD, CLAP-based scores) and subjective human listening tests (e.g., MOS, paired preference studies).
