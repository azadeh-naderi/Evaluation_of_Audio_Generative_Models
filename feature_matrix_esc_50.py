# -*- coding: utf-8 -*-
"""feature matrix_ESC_50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1baorMF6YlciMG4FMfIQYD4zIk_DbmgZW
"""

import pandas as pd
import csv
import librosa
import librosa.display
import numpy as np
import requests

"""try to use Soundata"""

!pip install soundata
#!pip install numpy

import soundata

# learn wich datasets are available in soundata
#print(soundata.list_datasets())


from soundata.datasets import ESC50

dataset.download()

"""Load the data"""

# Define the URL of the CSV file
csv_url = 'https://github.com/karolpiczak/ESC-50/raw/master/meta/esc50.csv'

# Read the CSV file from the provided URL
df = pd.read_csv(csv_url)


#df = pd.read_csv(csv_url, error_bad_lines=False)
#dog_df = df[df['category'] == 'dog']

# Print the first 5 rows of the DataFrame
#print(dog_df.head())

from io import BytesIO

def load_audio_data(row):
    # Construct the raw content URL for the audio file
    audio_file_url = f'https://github.com/karolpiczak/ESC-50/raw/master/audio/{row["filename"]}'

    # Download the audio file
    response = requests.get(audio_file_url)

    if response.status_code == 200:
        # Create a BytesIO object from the response content
        audio_data = BytesIO(response.content)

        # Load the audio from the BytesIO object
        y, sr = librosa.load(audio_data, sr=None)
        return y, sr

"""Comprehensive matrix (with for loop, slow):

rows: all the dataset (2000)

columns: mean (13),variance(13), delta(13), delta-delta(13) = 52
"""

def calculate_comprehensive_matrix(df):
    # Initialize empty list to store feature vectors
    features = []

    for index, row in df.iterrows():
        y, sr = load_audio_data(row)

        # Extract MFCC features
        mfcc = librosa.feature.mfcc(y=y, n_mfcc=13, sr=sr)

        # Calculate mean of MFCC
        mfcc_mean = np.mean(mfcc, axis=1)

        # Calculate variance of MFCC
        mfcc_variance = np.var(mfcc, axis=1)

        # Calculate delta of MFCC and the mean of the delta values
        mfcc_delta = librosa.feature.delta(mfcc)
        delta_mean = np.mean(mfcc_delta, axis=1)

        # Calculate delta-delta of MFCC and the mean of the delta-delta values
        mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)
        delta_delta_mean = np.mean(mfcc_delta_delta, axis=1)

        # Append the results to the respective lists
        feature_vector = np.concatenate((mfcc_mean, mfcc_variance, delta_mean, delta_delta_mean))

        features.append(feature_vector)

    # Convert the lists of feature vectors to a feature matrix
    comprehensive_matrix = np.array(features)

    return comprehensive_matrix




# Call the function with the DataFrame as an argument
comprehensive_matrix = calculate_comprehensive_matrix(df)

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)

"""faster function for comprehnsive matrix"""

from concurrent.futures import ThreadPoolExecutor

def calculate_comprehensive_matrix(df):
    # Define the number of MFCC features you want
    n_mfcc = 13

    # Initialize empty list to store feature vectors
    features = []

    # Function to extract features from a single row
    def extract_features(row):
        y, sr = load_audio_data(row[1])
        mfcc = librosa.feature.mfcc(y=y, n_mfcc=n_mfcc, sr=sr)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_variance = np.var(mfcc, axis=1)
        mfcc_delta = librosa.feature.delta(mfcc)
        delta_mean = np.mean(mfcc_delta, axis=1)
        mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)
        delta_delta_mean = np.mean(mfcc_delta_delta, axis=1)
        feature_vector = np.concatenate((mfcc_mean, mfcc_variance, delta_mean, delta_delta_mean))
        return feature_vector

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor(max_workers=6) as executor:
        features = list(executor.map(extract_features, df.iterrows()))

    # Convert the list of feature vectors to a feature matrix
    comprehensive_matrix = np.array(features)

    return comprehensive_matrix


# Call the function with the DataFrame as an argument
comprehensive_matrix = calculate_comprehensive_matrix(df)

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)

"""#### Sanity Check on Comprehensive Matrix"""

# Check the size of the comprehensive matrix.
# Num rows to get num files (one dimension)
# Num columns: 13 * 4 = 52 (features)

print(comprehensive_matrix.shape)

"""PCA"""

!pip install scikit-learn
from sklearn.decomposition import PCA

# Run PCA on comprehensive matrix
def pca(comprehensive_matrix, num_components=30):

    # Initialize PCA with the desired number of components
    pca = PCA(n_components=num_components,whiten = True)

    # Fit the PCA model to the data and transform the data
    reduced_features = pca.fit_transform(comprehensive_matrix)

    return reduced_features


# Call the function to run PCA with 2 components on comprehensive matrix
reduced_matrix = pca(comprehensive_matrix, num_components=30)


# Print the matrix
print("Reduced Matrix:")
print(reduced_matrix)

def class_variance(df, reduced_matrix, class_names):

    if not isinstance(class_names, list):
        class_names = [class_names]

    class_variances = []

    for class_name in class_names:
        class_indices = [i for i, label in enumerate(df["category"]) if label == class_name]
        class_variance = np.var(reduced_matrix[class_indices], axis=1)
        class_variances.append(class_variance)

    combined_variances = np.concatenate(class_variances, axis=0)
    total_variance = np.mean(combined_variances)

    return total_variance

class_name = ["dog"]
result = class_variance(df, reduced_matrix, class_name)
print(result)

class_name = "glass_breaking"
result = class_variance(df, reduced_matrix, class_name)
print(f"total variance for class/es '{class_name}': {result}")

class_name = ["dog", "glass_breaking"]
result = class_variance(df, reduced_matrix, class_name)
print(f"total variance for class/es '{class_name}': {result}")

def total_class_variance(df, reduced_matrix, class_name):

  class_indices = [i for i, label in enumerate(df["category"]) if label == class_name]
  #print(class_indices)
  class_variance = np.var(reduced_matrix[class_indices], axis=1)
  #print(class_variance)
  total_class_variance = np.mean(class_variance)
  return total_class_variance

# Example usage:
class_name = "airplane"
total_class_variance = class_variance(df, reduced_matrix, class_name)
print(total_class_variance)
#print(f"Variance for class '{class_name}': {class_variance}")
#print(f"Variance for class '{class_name}':")
#print(class_variance)

# Specify the class name you want to calculate the variance for
target_class_name = "dog"

# Calculate the total variance for the specified class
class_indices = [i for i, label in enumerate(df["category"]) if label == target_class_name]
print(class_indices)

#if len(class_indices) == 0:
    #print(f"No samples found for the class '{target_class_name}'.")
#else:
#class_variance = np.mean(np.var(reduced_matrix[class_indices], axis=1))
#print(f"Variance for class '{target_class_name}':")
#print(class_variance)

"""PCA on each class:

rows: data per each class (40)

columns: number of components (num_components=30)
"""

!pip install scikit-learn
from sklearn.decomposition import PCA



# Run PCA on comprehensive matrix for each class
def PCA_per_class(comprehensive_matrix, num_components=30, class_name=''):

    # Initialize a dictionary to store PCA results for each class
    pca_per_class = {}

    class_indices = (comprehensive_matrix[:, -1] == class_name)

    if class_indices.any():  # Check if there are data points for the class
        # Select the feature data for the specified class
        feature_data_per_class = comprehensive_matrix[class_indices]

        # Initialize PCA with the desired number of components
        pca = PCA(n_components=num_components, whiten=True)

        # Fit and transform the feature data for the current class using PCA
        reduced_features_per_class = pca.fit_transform(feature_data_per_class[:, :-1])

        # Store the PCA results for the current class
        pca_per_class = reduced_features_per_class

    return pca_per_class



# Call the function to run PCA on specific class on comprehensive matrix
reduced_features_per_class = PCA_per_class(comprehensive_matrix, 30, 'dog')


# Print the matrix
print("Reduced Matrix Per Class=dog:")
print(reduced_features_per_class)

reduced_matrix.shape

"""calculate variance per each individual classe and two classes combine:

NOTE: the dog variance was so high, we used (whiten = True) when running PCA
"""

var_dog = np.var(PCA_per_class(comprehensive_matrix, 30, 'dog'), axis=1)
print("variance for class=dog:")
print(var_dog)



var_rain = np.var(PCA_per_class(comprehensive_matrix, 30, 'rain'), axis=1)
print("variance for class=rain:")
print(var_rain)


var_dog_rain = np.concatenate([var_dog, var_rain])
print("variance for class=dog and rain:")
print(var_dog_rain)

var_Airplane = np.var(PCA_per_class(comprehensive_matrix, 30, 'Airplane'), axis=1)
print("variance for class=Airplane:")
print(var_Airplane)



var_Engine = np.var(PCA_per_class(comprehensive_matrix, 30, 'Engine'), axis=1)
print("variance for class=Engine:")
print(var_Engine)


var_Airplane_Engine = np.concatenate([var_Airplane, var_Engine])
print("variance for class=airplane and engine:")
print(var_Airplane_Engine)

"""Compare total variance for individual class and two classes combine:"""

total_var_dog = np.mean(var_dog)
print("total variance for class=dog:")
print(total_var_dog)


total_var_rain = np.mean(var_rain)
print("total variance for class=rain:")
print(total_var_rain)


total_var_dog_rain = np.mean(var_dog_rain)
print("total variance for class=dog and rain:")
print(total_var_dog_rain)