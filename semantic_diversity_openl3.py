# -*- coding: utf-8 -*-
"""Semantic diversity_openl3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFWB1I-c-GGZm0AyBeHNUiTYDRwk4h1y
"""

!pip install openl3
import pandas as pd
import csv
import numpy as np
import requests
import scipy
import openl3
import soundfile as sf
from scipy.signal import lfilter

"""Load the data"""

# Define the URL of the CSV file
csv_url = 'https://github.com/karolpiczak/ESC-50/raw/master/meta/esc50.csv'

# Read the CSV file from the provided URL
df = pd.read_csv(csv_url)

from io import BytesIO

def load_audio_data(row):
    # Construct the raw content URL for the audio file
    audio_file_url = f'https://github.com/karolpiczak/ESC-50/raw/master/audio/{row["filename"]}'

    # Download the audio file
    response = requests.get(audio_file_url)

    if response.status_code == 200:
        # Create a BytesIO object from the response content
        audio_data = BytesIO(response.content)

        # Load the audio from the BytesIO object
        y, sr = sf.read(audio_data)
        return y, sr

"""Comprehensive matrix"""

from concurrent.futures import ThreadPoolExecutor
import tensorflow as tf

dog_df = df[df['category'] == 'dog']

def comprehensive_matrix(dog_df):

    # Initialize empty list to store feature vectors
    features = []
    means = []
    vars = []

    # Function to extract features from a single row
    #def extract_features(row):

    for index, row in dog_df.iterrows():
        y, sr = load_audio_data(row)
        emb, ts= openl3.get_audio_embedding(y, sr)
          #y, sr = load_audio_data(row[1])
          #emb, ts= openl3.get_audio_embedding(y, sr)

        features_mean = np.mean(emb, axis=1)
        features_variance = np.var(emb, axis=1)
          #features_deltas = np.diff(emb, n=1, axis=1)
          #delta_mean = np.mean(features_deltas, axis=1)
          #features_delta_deltas = np.diff(emb, n=2, axis=1)
          #delta_delta_mean = np.mean(features_delta_deltas, axis=1)

          #feature_vector = np.concatenate((features_mean, features_variance, delta_mean, delta_delta_mean))
          #feature_vector = np.concatenate((features_mean, features_variance))
          # Append the results to the respective lists
          means.append(features_mean)
          vars.append(features_variance)

          # Convert the lists to NumPy arrays
      means = np.array(means)
      vars = np.array(vars)

      feature_vector = np.concatenate((means, vars))
         # return feature_vector

    # Use ThreadPoolExecutor for parallel processing
    #with ThreadPoolExecutor(max_workers=6) as executor:
        #features = list(executor.map(extract_features, df.iterrows()))



    # Convert the list of feature vectors to a feature matrix
    #comprehensive_matrix = np.array(features)

    return comprehensive_matrix




# Call the function
comprehensive_matrix = comprehensive_matrix(dog_df)

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)


print(comprehensive_matrix.shape)



"""PCA"""

!pip install scikit-learn
from sklearn.decomposition import PCA

def pca(comprehensive_matrix, num_components=None):

    # Initialize PCA
    pca = PCA(n_components=num_components,whiten = True)

    # Fit the PCA model to the data and transform the data
    reduced_features = pca.fit_transform(comprehensive_matrix)

    return reduced_features


# Call the function to run PCA on comprehensive matrix
reduced_matrix = pca(comprehensive_matrix, num_components=10)

# Print the matrix
print("Reduced Matrix:")
print(reduced_matrix)

reduced_matrix.shape

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


num_audio_files = 10

# Initialize empty lists to store features,  variances
variances = []
stds = []

# Loop to upload, load, and process audio files
for i in range(1, num_audio_files + 1):
    audio_files = f"/content/index_{i}.wav"

    # Load the audio file
    audio, sr = sf.read(audio_files)

    # Extract OpenL3 features
    embeddings, timestamps = openl3.get_audio_embedding(audio, sr)

    # Calculate cosine similarity between embeddings
    similarity_matrix = cosine_similarity(embeddings)

    # Calculate variance of cosine similarities
    semantic_diversity_variance = np.var(similarity_matrix)

    # Calculate standard deviation of cosine similarities
    semantic_diversity_std_deviation = np.std(similarity_matrix)

    # Append the results to the respective lists
    variances.append(semantic_diversity_variance)
    stds.append(semantic_diversity_std_deviation)

# Convert the lists to NumPy arrays
variances = np.array(variances)
stds = np.array(stds)

comprehensive_matrix = np.concatenate((variances, stds))

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


dog_df = df[df['category'] == 'dog']

#num_audio_files = 10

# Initialize empty lists to store features,  variances
variances = []
stds = []

# Loop to upload, load, and process audio files
for index, row in dog_df.iterrows():
    #audio_files = f"/content/index_{i}.wav"
    y, sr = load_audio_data(row)
    # Load the audio file
    #audio, sr = sf.read(y)

    # Extract OpenL3 features
    embeddings, timestamps = openl3.get_audio_embedding(audio, sr)

    # Calculate cosine similarity between embeddings
    similarity_matrix = cosine_similarity(embeddings)

    # Calculate variance of cosine similarities
    semantic_diversity_variance = np.var(similarity_matrix)

    # Calculate standard deviation of cosine similarities
    semantic_diversity_std_deviation = np.std(similarity_matrix)

    # Append the results to the respective lists
    variances.append(semantic_diversity_variance)
    stds.append(semantic_diversity_std_deviation)

# Convert the lists to NumPy arrays
variances = np.array(variances)
stds = np.array(stds)

comprehensive_matrix = np.concatenate((variances, stds))

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)

from concurrent.futures import ThreadPoolExecutor
import tensorflow as tf

dog_df = df[df['category'] == 'dog']

def comprehensive_matrix(dog_df):

    # Initialize empty list to store feature vectors
    variances = []
    stds = []
    deltas = []
    # Function to extract features from a single row
    #def extract_features(row):
    for index, row in dog_df.iterrows():
        y, sr = load_audio_data(row)
        emb, ts= openl3.get_audio_embedding(y, sr)

        features_mean = np.mean(emb, axis=1)
        features_variance = np.var(emb, axis=1)
        features_deltas = np.diff(emb, n=1, axis=1)
        delta_mean = np.mean(features_deltas, axis=1)
        #features_delta_deltas = np.diff(emb, n=2, axis=1)
        #delta_delta_mean = np.mean(features_delta_deltas, axis=1)

        # Append the results to the respective lists
        variances.append(features_mean)
        stds.append(features_variance)
        deltas.append(delta_mean)

    # Convert the lists to NumPy arrays
    variances = np.array(variances)
    stds = np.array(stds)
    deltas = np.array(deltas)
    comprehensive_matrix = np.concatenate((variances, stds))

    return comprehensive_matrix


# Call the function
comprehensive_matrix = comprehensive_matrix(dog_df)

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)


print(comprehensive_matrix.shape)

from concurrent.futures import ThreadPoolExecutor
import tensorflow as tf

dog_df = df[df['category'] == 'dog']

def comprehensive_matrix(dog_df):

    # Initialize empty list to store feature vectors
    features = []

    # Function to extract features from a single row
    #def extract_features(row):
    for index, row in dog_df.iterrows():
        y, sr = load_audio_data(row)
        emb, ts= openl3.get_audio_embedding(y, sr)

        features_mean = np.mean(emb, axis=1)
        features_variance = np.var(emb, axis=1)
        features_deltas = np.diff(emb, n=1, axis=1)
        delta_mean = np.mean(features_deltas, axis=1)
        features_delta_deltas = np.diff(emb, n=2, axis=1)
        delta_delta_mean = np.mean(features_delta_deltas, axis=1)

        # Append the results to the respective lists
        feature_vector = np.concatenate((features_mean, features_variance, delta_mean, delta_delta_mean))

        features.append(feature_vector)

    # Convert the lists of feature vectors to a feature matrix
    comprehensive_matrix = np.array(features)

    return comprehensive_matrix


# Call the function
comprehensive_matrix = comprehensive_matrix(dog_df)

# Print the matrix
print("Comprehensive Matrix:")
print(comprehensive_matrix)


print(comprehensive_matrix.shape)